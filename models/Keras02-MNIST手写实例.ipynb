{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras实现简单的手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考](http://www.cnblogs.com/yqtm/p/6924939.html)\n",
    "文中代码有点小bug,加以改正。顺带才了下数据集的坑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入需要的函数和包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从s3.amazonaws.com/img-datasets/mnist.npz下载数据太慢了。挂了代理，结果程序运行崩溃，只好写一个加载本地的文件函数\n",
    "def load_data(path='mnist.npz'):\n",
    "    f=np.load(path)\n",
    "    x_train,y_train=f['x_train'],f['y_train']\n",
    "    x_test,y_test=f['x_test'],f['y_test']\n",
    "    f.close()\n",
    "    return (x_train,y_train),(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential是序贯模型，Dense是用于添加模型的层数，SGD是用于模型变异的时候优化器参数,\n",
    "mnist是用于加载手写识别的数据集，需要在网上下载,下面是mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from ..utils.data_utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(path='mnist.npz'):\n",
    "    \"\"\"Loads the MNIST dataset.\n",
    "\n",
    "    # Arguments\n",
    "        path: path where to cache the dataset locally\n",
    "            (relative to ~/.keras/datasets).\n",
    "\n",
    "    # Returns\n",
    "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "    path = get_file(path, origin='https://s3.amazonaws.com/img-datasets/mnist.npz')\n",
    "    f = np.load(path)\n",
    "    x_train, y_train = f['x_train'], f['y_train']\n",
    "    x_test, y_test = f['x_test'], f['y_test']\n",
    "    f.close()\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(500,input_shape=(784,)))#输入层\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))#隐藏层\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dense()设定该层的结构，第一个参数表示输出的个数，第二个参数是接受的输入数据的格式。第一层中需要指定输入的格式，在之后的增加的层中输入层节点数默认是上一层的输出个数\n",
    "2. Activation()指定预定义激活函数：softmax，elu、softplus、softsign、relu、、sigmoid、hard_sigmoid、linear<br>\n",
    "3. Dropout()用于指定每层丢掉的信息百分比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd=SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True)#设定学习效率等参数\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd)\n",
    "#model.compile(loss = 'categorical_crossentropy', optimizer=sgd, class_mode='categorical') #使用交叉熵作为loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用model.compile()之前初始化一个优化器对象，然后传入该函数,使用优化器sgd来编译模型，用来指定学习效率等参数。编译时指定loss函数，这里使用交叉熵函数作为loss函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SGD*\n",
    "\n",
    "```\n",
    "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "```\n",
    "随机梯度下降法，支持动量参数，支持学习衰减率，支持Nesterov动量\n",
    "\n",
    "参数\n",
    "\n",
    "- `lr`：大于0的浮点数，学习率\n",
    "- `momentum`：大于0的浮点数，动量参数\n",
    "- `decay`：大于0的浮点数，每次更新后的学习率衰减值\n",
    "- `nesterov`：布尔值，确定是否使用Nesterov动量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=load_data()#直接加载本地文件\n",
    "#(x_train,y_train),(x_test,y_test)=mnist.load_data()#不使用mnist提供的load_data函数，\n",
    "X_train=x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\n",
    "X_test=x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])\n",
    "Y_train=(np.arange(10)==y_train[:,None]).astype(int)#将index转换成一个one_hot矩阵\n",
    "Y_test=(np.arange(10)==y_test[:,None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "[[[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " ..., \n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]]\n",
      "(10000, 28, 28)\n",
      "y_train: [5 0 4 ..., 5 6 8] 60000\n",
      "[5 0 4 ..., 5 6 8]\n",
      "[[False False False ..., False False False]\n",
      " [ True False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False  True False]]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train)\n",
    "print(x_test.shape)\n",
    "print(\"y_train:\",y_train,len(y_train))\n",
    "print(y_train[:None])\n",
    "print(y_train[:,None]==np.arange(10))\n",
    "print(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 读取minst数据集，通过reshape()函数转换数据的格式。\n",
    "2. 如果我们打印x_train.shape会发现它是(60000,28,28)，即一共60000个数据，每个数据是28*28的图片。通过reshape转换为(60000,784)的线性张量。\n",
    "3. 如果我们打印y_train会发现它是一组表示每张图片的表示数字的数组，通过numpy的arange()和astype()函数将每个数字转换为一组长度为10的张量，代表的数字的位置是1，其它位置为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 5s - loss: 1.2457 - val_loss: 0.5666\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.9481 - val_loss: 0.4958\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.8623 - val_loss: 0.4659\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.8145 - val_loss: 0.4691\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.7788 - val_loss: 0.4342\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.7225 - val_loss: 0.4105\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.7338 - val_loss: 0.3970\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6848 - val_loss: 0.3961\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6693 - val_loss: 0.3875\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6544 - val_loss: 0.3751\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6276 - val_loss: 0.3681\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6605 - val_loss: 0.3660\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6487 - val_loss: 0.3515\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6426 - val_loss: 0.3646\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6292 - val_loss: 0.3424\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.6074 - val_loss: 0.3378\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5844 - val_loss: 0.3320\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5753 - val_loss: 0.3363\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5570 - val_loss: 0.3199\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5452 - val_loss: 0.3108\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5320 - val_loss: 0.3108\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5354 - val_loss: 0.3024\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5172 - val_loss: 0.2973\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5222 - val_loss: 0.3037\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5208 - val_loss: 0.2940\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5154 - val_loss: 0.2948\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5258 - val_loss: 0.2918\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.5033 - val_loss: 0.2889\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4962 - val_loss: 0.2828\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4848 - val_loss: 0.2761\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4884 - val_loss: 0.2881\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4873 - val_loss: 0.2794\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4823 - val_loss: 0.2686\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4781 - val_loss: 0.2788\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4781 - val_loss: 0.2732\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4786 - val_loss: 0.2880\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4829 - val_loss: 0.2729\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4744 - val_loss: 0.2731\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4564 - val_loss: 0.2698\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4614 - val_loss: 0.2629\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4673 - val_loss: 0.2586\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4666 - val_loss: 0.2524\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4545 - val_loss: 0.2682\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4550 - val_loss: 0.2653\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4426 - val_loss: 0.2537\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4322 - val_loss: 0.2523\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4541 - val_loss: 0.2552\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4465 - val_loss: 0.2493\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4366 - val_loss: 0.2445\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4362 - val_loss: 0.2458\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4388 - val_loss: 0.2446\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4440 - val_loss: 0.2551\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4278 - val_loss: 0.2469\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4185 - val_loss: 0.2416\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4086 - val_loss: 0.2332\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4005 - val_loss: 0.2407\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4064 - val_loss: 0.2396\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4063 - val_loss: 0.2384\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4020 - val_loss: 0.2358\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4008 - val_loss: 0.2332\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4045 - val_loss: 0.2338\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4153 - val_loss: 0.2346\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4102 - val_loss: 0.2279\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.4013 - val_loss: 0.2337\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3945 - val_loss: 0.2312\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3917 - val_loss: 0.2243\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3780 - val_loss: 0.2219\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3781 - val_loss: 0.2249\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3755 - val_loss: 0.2192\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3814 - val_loss: 0.2164\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3843 - val_loss: 0.2197\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3835 - val_loss: 0.2228\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3908 - val_loss: 0.2281\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3881 - val_loss: 0.2185\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3870 - val_loss: 0.2108\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3731 - val_loss: 0.2112\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3685 - val_loss: 0.2069\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3633 - val_loss: 0.2059\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3626 - val_loss: 0.2073\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3594 - val_loss: 0.2053\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3489 - val_loss: 0.2001\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3521 - val_loss: 0.2007\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3488 - val_loss: 0.2029\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3531 - val_loss: 0.1984\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s - loss: 0.3545 - val_loss: 0.2034\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3559 - val_loss: 0.2053\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3551 - val_loss: 0.2019\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3538 - val_loss: 0.2043\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3498 - val_loss: 0.2050\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3566 - val_loss: 0.2076\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3573 - val_loss: 0.2052\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3633 - val_loss: 0.1994\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3561 - val_loss: 0.2004\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3473 - val_loss: 0.2015\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3463 - val_loss: 0.1951\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3485 - val_loss: 0.1985\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3357 - val_loss: 0.1994\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3399 - val_loss: 0.1965\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3408 - val_loss: 0.1931\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 4s - loss: 0.3366 - val_loss: 0.1956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a5fdb3d278>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,batch_size=200,epochs=100,shuffle=True,verbose=1,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size表示每个训练块包含的数据个数，\n",
    "- epochs表示训练的次数，\n",
    "- shuffle表示是否每次训练后将batch打乱重排，\n",
    "- verbose表示是否输出进度log，\n",
    "- validation_split指定验证集占比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set\n",
      " 8800/10000 [=========================>....] - ETA: 0s\n",
      "The test loss is 0.185958\n",
      "10000/10000 [==============================] - 0s     \n",
      "\n",
      "The accuracy of the model is 0.943400\n"
     ]
    }
   ],
   "source": [
    "print(\"test set\")\n",
    "scores = model.evaluate(X_test,Y_test,batch_size=200,verbose=1)\n",
    "print(\"\")\n",
    "print(\"The test loss is %f\" % scores)\n",
    "result = model.predict(X_test,batch_size=200,verbose=1)\n",
    "\n",
    "result_max = np.argmax(result, axis = 1)\n",
    "test_max = np.argmax(Y_test, axis = 1)\n",
    "\n",
    "result_bool = np.equal(result_max, test_max)\n",
    "true_num = np.sum(result_bool)\n",
    "print(\"\")\n",
    "print(\"The accuracy of the model is %f\" % (true_num/len(result_bool)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model.evaluate()计算了测试集中的识别的loss值。\n",
    "- 通过model.predict()，我们可以得到对于测试集中每个数字的识别结果，每个数字对应一个表示每个数字都是多少概率的长度为10的张量。\n",
    "- 通过np.argmax()，我们得到每个数字的识别结果和期望的识别结果\n",
    "\n",
    "- 通过np.equal()，我们得到每个数字是否识别正确\n",
    "\n",
    "- 通过np.sum()得到识别正确的总的数字个数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
